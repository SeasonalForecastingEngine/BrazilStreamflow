{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduksjon til PyTorch\n",
    "## Standardfunksjoner\n",
    "For å bruke Pytoch må vi laste pakken `torch`. Numpy kommer også til å være praktisk å ha tilgjengelig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x732d861046f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(1444)\n",
    "torch.manual_seed(1444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch har mye til felles med numpy, men stor forskjell på de to pakkene er at mens numpy bruker array-er bruker torch tensorer. Disse er ulike datatyper, men de oppfører seg ekstremt likt. Man kan også lette bytte fram og tilbake mellom tensorer og array-er med `.numpy()`og `torch.from_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True])\n",
      "[ True  True  True]\n",
      "tensor([True, True, True])\n"
     ]
    }
   ],
   "source": [
    "x_array = np.array([1,2,3]) #Numpy array\n",
    "x_tensor = torch.tensor([1,2,3]) #Tensor\n",
    "print(x_array == x_tensor)\n",
    "\n",
    "#Gå fra numpy til torch og fra torch til numpy\n",
    "print(x_array == x_tensor.numpy()) #.numpy() tar deg fra torch til numpy\n",
    "print(x_tensor == torch.from_numpy(x_array)) #torch.from_numpy tar deg fra numpy til torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy og torch oppfører seg veldig likt og mange kommandoer er veldig like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6478, 0.7097],\n",
       "        [0.6743, 0.8946]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lage tensorer\n",
    "x = torch.tensor([[1,2], [3,4]]) #Lager en tensor variant av matrisen med 1,2 i første og 3,4 i andre rad\n",
    "x = torch.tensor([[1,2],[3,4]], dtype = torch.float) #Lager en matrise som over, men sørger for at typen er satt til float\n",
    "x = torch.ones(3) #Lager en (3,) tensor med 1 i alle entries\n",
    "x = torch.zeros(3) #Lager en (3,) tensor med 0 i alle entries\n",
    "x = torch.linspace(0,1,100) #Lager en (100,) tensor med entries mellom 0 og 1 som alle er like langt unna hverandre\n",
    "x = torch.arange(0,1,0.01) #Lager en tensor med entries mellom 0 og 1 som alle har avstand 0.01 til hverandre\n",
    "x = torch.rand(5) #Lager en vektor med entries trukket fra U[0,1]-fordelinga\n",
    "x = torch.randn(2) #Lager en vektor med entries trukket fra N(0,1)-fordelinga\n",
    "\n",
    "#shape og datatype\n",
    "x.dtype #Datatype til en tensor\n",
    "x.shape #Shape til en tensor\n",
    "\n",
    "#Lin-alg funksjoner\n",
    "A = torch.rand((2,2))\n",
    "x = torch.rand(2)\n",
    "A @ x #matrisemultiplikasjon\n",
    "torch.matmul(A, x) #Alternativ syntaks for A @ x\n",
    "x*x #Entrywise multiplikasjon\n",
    "x+x #Entrywise addisjon\n",
    "A.T #Transponering\n",
    "\n",
    "#Vanlige funksjoner\n",
    "torch.log(x) #naturlig logaritme\n",
    "torch.exp(x) #eksponensialfunksjon\n",
    "torch.sin(x) #sinus, tilsvarende for cos, tan osv\n",
    "torch.sum(x) #summere entries\n",
    "torch.mean(x) #Ta gjennomsnitt over entries\n",
    "torch.diag(x) #Lag en matrise med passende dimensjoner og putt x på diagonalen\n",
    "torch.diag(A) #Hent ut diagonalen til A\n",
    "\n",
    "#Slicing og elementer\n",
    "A[0,1] #Henter ut entry (0,1) fra A\n",
    "A[:,0] #Henter ut første kollonne fra A\n",
    "A[x>0.3,:] #Henter ut alle rader der korresponderende entry i x er større enn 0.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kan også som regel sende tensorer inn til numpy funksjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37221/1277889159.py:1: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  np.log(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0892, -0.6188])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kan enkelt lage nye vektorer med samme dimensjon som en annen ved å legge til _like til mange funksjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2763,  0.9976],\n",
       "        [ 0.5432, -0.0929]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(A)\n",
    "torch.zeros_like(A)\n",
    "torch.rand_like(A)\n",
    "torch.randn_like(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Å slå sammen tensorer er litt knotete, men ikke noe mer enn i numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1,1],[1,1]])\n",
    "B = torch.zeros((3,2))\n",
    "torch.cat([A,B], 0) #Legger B til under A\n",
    "torch.cat([B.T,A], 1) #Legger A til til venstre for B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at cat skal fungere, må dimensjonene matche. En 1-dim tensor av lengde d har formelt shape (d,) og må derfor reshapes for å kunne legges til en (d,d)-matrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2)\n",
    "torch.cat([A,x.reshape(1,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatisk derivering\n",
    "\n",
    "Noe av det som gjør PyTorch så utrolig nyttig er at det bruker noe som heter automatisk derivering. Automatisk derivering er en måte å få datamaskinen til å regne ut den deriverte til funksjoner. Dette blir gjort på en analytisk måte, så svaret man får er **ikke** en tilnærming slik som i nummerisk derivering, men faktisk den sanne verdien av den deriverte.\n",
    "\n",
    "Automatisk derivering fungerer ved å utnytte kjernereglen for alt den er verdt. Hvis en funksjon $f(x)$ kan skrives som en komposisjon av to andre funksjoner: $f(x) = g[h(x)]$, sier kjernereglen at $f'(x) = g'[h(x)]h'(x)$. Hvis man kjenner $g'[h(x)]$ og $h'(x)$ kan man derfor finne $f'(x)$ uten å regne på utrykket for $f$ ved å bruke kjernereglen. Dette er teoremet som får automatisk derivering til å fungere.\n",
    "\n",
    "Enhver funksjon som er implementert på datamaskinen din er i bunn og grunn en lang komposisjon av enkle operasjoner som +, -, $\\times$, / og ^. For eksempel kan $f(x) = 2x^2 + 3$ skrives som $f(x) = g\\{h[k(x)]\\}$ der $g(x) = x+3$, $h(x) = 2x$ og $k(x) = x^2$, et mer komplisert eksempel er $\\log x$ som regnes ut av datamaskinen ved hjelp av en Taylor-tilnærming. Generelt kan vi skrive mer eller mindre alle funksjoner som $f(x) = g_n(g_{n-1}(...g_1(x)))$ der hver $g_i$ er en \"enkel\" operasjon. Pakkene som utfører automatisk derivering  har implementert utrykk for de deriverte til enkle operasjoner og holder styr på alle komposisjonene i funksjonene implementert på datamaskinen, slik at deres deriverte kan regnes ut automatisk ved hjelp av kjerneregelen!\n",
    "\n",
    "**!**\n",
    "Pytorch bruker pakken Autograd til å regne ut deriverte. Denne pakken kan man laste ned uten å laste ned PyTorch og den er mye enklere å bruke. Hvis kunn atomatisk derivering er av interesse, anbefaler jeg derfor å sjekke ut denne pakken.\n",
    "\n",
    "\n",
    "For at automatisk derivering skal fungere, må datamaskinen holde styr på alle operasjonene som brukes for å regne ut f(x). Vi må fortelle maskinen at den skal gjøre dette. Alle tensorer har en egenskap `requires_grad` som forteller PyTorch hvorvidt den skal holde styr på alt som gjøres med tensoren eller ikke. Hvis vi vil derivere med hensyn på en tensor, må vi derfor sette denne egenskapen til `True` for denne tensoren \n",
    "\n",
    "\n",
    "Automatisk derivering fungerer bare for å regne ut verdien av den deriverte i hvert punkt. Det er ikke symbolsk derivering som prøver å evaluere utrykk og gi en formel for den deriverte (slik som i wolfram alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, requires_grad = True) #Sett argumentet til True når tensoren lages\n",
    "x = torch.ones(3)\n",
    "x.requires_grad = True #Sett argumentet til False etterpå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den deriverte til en funksjon kan regnes ut ved hjelp av `torch.autograd.grad`. Denne funksjonen returnerer et tuppel med svaret i første argument og ingenting i det andre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([7.2832]),)\n",
      "tensor([7.2832])\n"
     ]
    }
   ],
   "source": [
    "#Definerer en funksjon å derivere\n",
    "def f(x):\n",
    "    return torch.sin(2*torch.pi*x) + torch.log(x)\n",
    "\n",
    "x = torch.ones(1, requires_grad = True) #Vi deriverer i x = 1\n",
    "\n",
    "#Regn ut den deriverte med automatisk derivering\n",
    "auto_diff = torch.autograd.grad(outputs=f(x),\n",
    "                                inputs = x)\n",
    "#Regn ut den deriverte analytisk\n",
    "analytical_diff = torch.tensor([2*torch.pi*torch.cos(2*torch.pi*x) + 1/x])\n",
    "\n",
    "#Print svar\n",
    "print(auto_diff)\n",
    "print(analytical_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis $f$ er vektorvaluert er $f'(x)$ en matrise og ikke en vektor. Da er det Jacobimatrisen man ønsker å regne ut. Dette kan man gjøre ved å bruke `torch.autograd.functional.jacobian`. Det er også en funksjon for å regne ut Hessematriser i `torch.autograd.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2832, 0.0000],\n",
       "        [0.0000, 7.2832]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Regner ut Hesse matrisen til f\n",
    "torch.autograd.functional.hessian(f,\n",
    "                                  inputs = x)\n",
    "\n",
    "#Regn ut en Jacobimatrise av en vektorvaluert funksjon\n",
    "x = torch.ones(2, requires_grad=True)\n",
    "def f(x):\n",
    "    return torch.sin(2*torch.pi*x) + torch.log(x)\n",
    "\n",
    "torch.autograd.functional.jacobian(f,\n",
    "                                   inputs = x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksempel\n",
    "Automatisk derivering er implementert i PyTorch for å sørge for at modelltilpassingene av svært kompliserte modeller som nevrale nett går fort, men det kan være veldig praktisk for klassisk statistikk også, da det kan brukes for å gjøre optimeringsalgoritmer raskere og for å enkelt regne ut Fisher-matriser. Tettheter og lignende for mange vanlige statistiske modeller kan finnes på [PyTorch sine sider](https://pytorch.org/docs/stable/distributions.html#module-torch.distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tilnærmet varians for ML-estimator er:\n",
      "tensor([[1.0633, 0.6630],\n",
      "        [0.6630, 0.7405]])\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#Simmuler data fra Gamma(2,3)\n",
    "np.random.seed(1444)\n",
    "n = 50\n",
    "x = torch.from_numpy(stats.gamma.rvs(size = n, a = 1, scale = 2))\n",
    "\n",
    "#Definerer log-likelihood\n",
    "def neg_log_lik(theta):\n",
    "    dist = torch.distributions.gamma.Gamma(theta[0], theta[1])\n",
    "    return -torch.sum(dist.log_prob(x))\n",
    "\n",
    "#Definerer deriverte og hessematriser\n",
    "jac = lambda x: torch.autograd.functional.jacobian(neg_log_lik, torch.from_numpy(x))\n",
    "hess = lambda x: torch.autograd.functional.hessian(neg_log_lik, torch.from_numpy(x))\n",
    "\n",
    "#Finner ML-estimat\n",
    "mini = minimize(neg_log_lik,\n",
    "                x0 = torch.ones(2),\n",
    "                jac = jac,\n",
    "                hess = hess,\n",
    "                method = \"Newton-CG\")\n",
    "\n",
    "#Tilnærmet varians\n",
    "J = hess(mini.x)/n #Tilnærmet Fishermatrise\n",
    "print(\"Tilnærmet varians for ML-estimator er:\")\n",
    "print(np.linalg.inv(J))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scriptet over tilpasser en gamma-fordeling til simmulert data og regner ut Fisher-matrisen uten at deriverte må tilnærmes eller regnes ut for hånd!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativ syntaks\n",
    "Ovenfor har vi brukt pakken autograd (som er inkludert i PyTorch) direkte til å finne deriverte. Denne måten å gjøre ting på er relativt rett fram, men krever en del notasjon. PyTorch lar deg derfor regne ut deriverte på en enda enklere måte.\n",
    "\n",
    "Hvis man setter argumentet `requires_grad` til en tensor `x` til `True`, vil PyTorch automatisk holde styr på alt som blir gjort med denne tensoren og deriverte kan hentes ut fortløpende. For at dette skal fungerer må vi imidlertid først fortelle PyTorch hva vi vil derivere med hensyn på `x`. Dette gjør man ved å kalle på `.backward()` som regner ut alle deriverte til tensoren man bruker den på. Gradientene kan hentes ut ved å hente ut `.grad` fra den tensoren man deriverer med hensyn på."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 3*x**2\n",
    "\n",
    "x.grad #Vi har ikke bedt PyTorch regne ut gradienter ennå, så denne er tom\n",
    "\n",
    "#Regn ut gradienter\n",
    "y.backward()\n",
    "\n",
    "x.grad #Nå er partila y/partial x innerholdt i x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi gjør flere ting med `x` trenger, men bare kaller `.backward()` på en av operasjonene blir bare denne operasjonen derivert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 3*x**2\n",
    "z = torch.sin(x)\n",
    "\n",
    "#Regn ut gradienter\n",
    "y.backward()\n",
    "\n",
    "x.grad #Nå er partila y/partial x innerholdt i x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan regne ut deriverte med hensyn på flere variabler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "y = 3*x**2 + torch.sin(z)\n",
    "\n",
    "#Regn ut gradienter\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(z.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
